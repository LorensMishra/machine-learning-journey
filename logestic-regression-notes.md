# ğŸ“˜ Logistic Regression â€” Detailed Theory

## ğŸ“Œ What is Logistic Regression?

**Logistic Regression** is a **supervised machine learning algorithm** used for **classification problems**, mainly **binary classification**.

Examples:

* Spam âŒ / Not Spam âœ…
* Pass âŒ / Fail âœ…
* Disease âŒ / No Disease âœ…

Despite the name, it is used for **classification**, not regression.

---

## ğŸ§  Core Idea (Intuition)

Linear regression gives output from **â€“âˆ to +âˆ**
But classification needs output **between 0 and 1**

So we use the **Sigmoid (Logistic) Function**.

---

## 1ï¸âƒ£ Why Logistic Regression Exists

Linear Regression predicts **continuous values**:
[
y = wx + b
]

Problem:

* Output can be **< 0 or > 1**
* Cannot represent **probability**

But classification problems need:
[
0 \le P(y=1|x) \le 1
]

ğŸ‘‰ **Solution:** Pass linear output through a **sigmoid function**.

---

## 2ï¸âƒ£ Sigmoid (Logistic) Function â€” Heart of the Model

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

### Properties:

* Output range: **(0, 1)**
* S-shaped curve
* Differentiable (important for gradient descent)

### Behavior:

* Large positive `z` â†’ output â‰ˆ 1
* Large negative `z` â†’ output â‰ˆ 0
* `z = 0` â†’ output = 0.5

---

## 3ï¸âƒ£ Logistic Regression Model Equation

### Step 1: Linear combination

[
z = w_1x_1 + w_2x_2 + \dots + w_nx_n + b
]

### Step 2: Apply sigmoid

[
\hat{y} = \sigma(z)
]

Here:

* `Å·` = predicted probability that class = 1
* `w` = weights
* `b` = bias

---

## 4ï¸âƒ£ Probabilistic Interpretation (Very Important)

Logistic regression models:
[
P(y=1|x) = \sigma(z)
]

And:
[
P(y=0|x) = 1 - \sigma(z)
]

So output is **probability**, not just class label.

---

## 5ï¸âƒ£ Decision Boundary

The decision boundary occurs when:
[
P(y=1|x) = 0.5
]

Which means:
[
z = 0
\Rightarrow wx + b = 0
]

ğŸ‘‰ This forms a **linear decision boundary** (line / plane / hyperplane).

---

## 6ï¸âƒ£ Why NOT Mean Squared Error (MSE)?

Using MSE with sigmoid:

* Cost function becomes **non-convex**
* Multiple local minima
* Slow & unstable training

ğŸ‘‰ Hence, **Log Loss** is used.

---

## 7ï¸âƒ£ Cost Function (Log Loss / Binary Cross Entropy)

[
J(w) = -\frac{1}{m} \sum_{i=1}^{m}
\Big[y_i \log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)\Big]
]

### Intuition:

* Strong penalty for confident wrong predictions
* Convex function â†’ single global minimum

---

# ğŸ“‚ File-Structure GitHub repo

Create file:

```
machine-learning-journey/
â”œâ”€â”€ logistic_regression_sklearn.py
â”œâ”€â”€ logistic_regression_from_scratch.py
â””â”€â”€ logestic-regression-notes.md

```

## 8ï¸âƒ£ Training Logistic Regression

### Optimization Algorithm:

* **Gradient Descent**
* **Stochastic Gradient Descent**
* **Newtonâ€™s Method** (used internally by some solvers)

### Weight Update Rule:

[
w = w - \alpha \frac{\partial J}{\partial w}
]

Where:

* `Î±` = learning rate
* `âˆ‚J/âˆ‚w` = gradient

---

## 9ï¸âƒ£ Regularization (Overfitting Control)

### Why needed?

* High-dimensional data
* Noisy features
* Prevents large weights

---

### ğŸ”¹ L2 Regularization (Ridge)

[
J = \text{Log Loss} + \lambda \sum w^2
]

* Shrinks weights
* Keeps all features

---

### ğŸ”¹ L1 Regularization (Lasso)

[
J = \text{Log Loss} + \lambda \sum |w|
]

* Forces some weights to **zero**
* Feature selection

---

## ğŸ”Ÿ Multiclass Logistic Regression

### One-Vs-Rest (OvR)

* Train `k` binary classifiers
* Choose class with highest probability

### Softmax Regression (Multinomial)

[
P(y=j|x) = \frac{e^{z_j}}{\sum e^{z_k}}
]

Used when:

* More than 2 classes
* Classes are mutually exclusive

---

## 1ï¸âƒ£1ï¸âƒ£ Assumptions of Logistic Regression

âœ” Binary dependent variable
âœ” Independent observations
âœ” No multicollinearity
âœ” Linear relationship between features and log-odds

---

## 1ï¸âƒ£2ï¸âƒ£ Log-Odds (Advanced Theory â€“ Interview Favorite)

[
\log\left(\frac{P(y=1)}{P(y=0)}\right) = wx + b
]

This means:

* Logistic regression is **linear in log-odds**
* Explains why itâ€™s interpretable

---

## 1ï¸âƒ£3ï¸âƒ£ Evaluation Metrics (Important)

Accuracy is not enough âŒ

| Metric    | Use                    |
| --------- | ---------------------- |
| Precision | False positives matter |
| Recall    | False negatives matter |
| F1-Score  | Balance                |
| ROC-AUC   | Probability ranking    |

---

## 1ï¸âƒ£4ï¸âƒ£ Advantages & Limitations (Theory Answer)

### Advantages

âœ” Simple
âœ” Fast
âœ” Interpretable
âœ” Probabilistic output

### Limitations

âŒ Linear boundary
âŒ Sensitive to outliers
âŒ Needs feature scaling

---

## ğŸ“Œ One-Line Exam Definition

> **Logistic Regression is a supervised classification algorithm that uses the sigmoid function to model the probability of a binary outcome and is trained using log-loss.**

---
